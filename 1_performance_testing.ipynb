{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Performance Testing Notebook\n",
    "\n",
    "This notebook is designed for conducting performance tests and analyzing the execution time of different operations (both in Pandas and PySpark)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import utils\n",
    "import test_functions as test\n",
    "\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Pandas Dataframes \n",
    "execution time: 37 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load small step data\n",
    "small_step_pd = []\n",
    "for i in range(10_000, 100_001, 10_000):\n",
    "    df = pd.read_csv(f\"data/small/pandas_test_{i}_rows.csv\")\n",
    "    small_step_pd.append(df)\n",
    "\n",
    "# load large step data\n",
    "large_step_pd = []\n",
    "for i in range(50_000, 1_000_001, 50_000):\n",
    "    df = pd.read_csv(f\"data/large/pandas_test_{i}_rows.csv\")\n",
    "    large_step_pd.append(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Spark Dataframes \n",
    "execution time: 28 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/jonas/opt/anaconda3/envs/PySpark_comp/lib/python3.7/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/07/13 11:01:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Performance Analysis\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# load small step data\n",
    "small_step_spark = []\n",
    "for i in range(10_000, 100_001, 10_000):\n",
    "    df = spark.read.csv(f\"data/small/pandas_test_{i}_rows.csv\", header=True, inferSchema=True, sep=\",\")\n",
    "    small_step_spark.append(df)\n",
    "\n",
    "# load large step data\n",
    "large_step_spark = []\n",
    "for i in range(50_000, 1_000_001, 50_000):\n",
    "    df = spark.read.csv(f\"data/large/pandas_test_{i}_rows.csv\", header=True, inferSchema=True)\n",
    "    large_step_spark.append(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Tests"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 1: Write Dataframe to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 20)\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "(20000, 20)\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "(30000, 20)\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "(40000, 20)\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "(50000, 20)\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "(60000, 20)\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "(70000, 20)\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "(80000, 20)\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "(90000, 20)\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "(100000, 20)\n",
      "pandas_df to csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/03 22:07:53 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_df to csv\n",
      "pandas_df to csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/03 22:07:56 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_df to csv\n",
      "pandas_df to csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/03 22:07:59 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_df to csv\n",
      "pandas_df to csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/03 22:08:02 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_df to csv\n",
      "pandas_df to csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/03 22:08:05 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_df to csv\n"
     ]
    }
   ],
   "source": [
    "# load time small statistics\n",
    "time_statistics_small = pd.read_csv(\"data/time_statistics_small.csv\")\n",
    "\n",
    "# run test\n",
    "time_statistics_small = utils.test_run(test.write_data, \"write_pd\", \"write_spark\", time_statistics_small, small_step_pd, small_step_spark, spark, 5)\n",
    "\n",
    "# save time small statistics\n",
    "time_statistics_small.to_csv(\"data/time_statistics_small.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(850000, 20)\n",
      "pandas_df to csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/03 22:47:44 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "23/07/03 22:47:44 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "23/07/03 22:47:44 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 76,00% for 10 writers\n",
      "23/07/03 22:47:44 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 69,09% for 11 writers\n",
      "23/07/03 22:47:44 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 63,33% for 12 writers\n",
      "23/07/03 22:47:44 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 58,46% for 13 writers\n",
      "23/07/03 22:47:44 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 54,29% for 14 writers\n",
      "23/07/03 22:47:44 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 50,67% for 15 writers\n",
      "23/07/03 22:47:44 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 47,50% for 16 writers\n",
      "23/07/03 22:47:45 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 50,67% for 15 writers\n",
      "23/07/03 22:47:46 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 54,29% for 14 writers\n",
      "23/07/03 22:47:46 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 58,46% for 13 writers\n",
      "23/07/03 22:47:46 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 63,33% for 12 writers\n",
      "23/07/03 22:47:46 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 69,09% for 11 writers\n",
      "23/07/03 22:47:46 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 76,00% for 10 writers\n",
      "23/07/03 22:47:46 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "23/07/03 22:47:46 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_df to csv\n",
      "pandas_df to csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/03 22:48:05 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "23/07/03 22:48:05 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "23/07/03 22:48:05 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 76,00% for 10 writers\n",
      "23/07/03 22:48:05 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 69,09% for 11 writers\n",
      "23/07/03 22:48:05 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 63,33% for 12 writers\n",
      "23/07/03 22:48:05 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 58,46% for 13 writers\n",
      "23/07/03 22:48:05 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 54,29% for 14 writers\n",
      "23/07/03 22:48:05 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 50,67% for 15 writers\n",
      "23/07/03 22:48:05 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 47,50% for 16 writers\n",
      "23/07/03 22:48:06 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 50,67% for 15 writers\n",
      "23/07/03 22:48:07 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 54,29% for 14 writers\n",
      "23/07/03 22:48:07 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 58,46% for 13 writers\n",
      "23/07/03 22:48:07 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 63,33% for 12 writers\n",
      "23/07/03 22:48:07 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 69,09% for 11 writers\n",
      "23/07/03 22:48:07 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 76,00% for 10 writers\n",
      "23/07/03 22:48:07 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "23/07/03 22:48:07 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_df to csv\n",
      "pandas_df to csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/03 22:48:25 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "23/07/03 22:48:25 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "23/07/03 22:48:25 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 76,00% for 10 writers\n",
      "23/07/03 22:48:25 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 69,09% for 11 writers\n",
      "23/07/03 22:48:25 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 63,33% for 12 writers\n",
      "23/07/03 22:48:25 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 58,46% for 13 writers\n",
      "23/07/03 22:48:25 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 54,29% for 14 writers\n",
      "23/07/03 22:48:25 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 50,67% for 15 writers\n",
      "23/07/03 22:48:25 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 47,50% for 16 writers\n",
      "23/07/03 22:48:26 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 50,67% for 15 writers\n",
      "23/07/03 22:48:27 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 54,29% for 14 writers\n",
      "23/07/03 22:48:27 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 58,46% for 13 writers\n",
      "23/07/03 22:48:27 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 63,33% for 12 writers\n",
      "23/07/03 22:48:27 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 69,09% for 11 writers\n",
      "23/07/03 22:48:27 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 76,00% for 10 writers\n",
      "23/07/03 22:48:27 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "23/07/03 22:48:27 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_df to csv\n",
      "pandas_df to csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/03 22:48:46 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "23/07/03 22:48:46 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "23/07/03 22:48:46 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 76,00% for 10 writers\n",
      "23/07/03 22:48:46 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 69,09% for 11 writers\n",
      "23/07/03 22:48:46 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 63,33% for 12 writers\n",
      "23/07/03 22:48:46 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 58,46% for 13 writers\n",
      "23/07/03 22:48:46 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 54,29% for 14 writers\n",
      "23/07/03 22:48:46 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 50,67% for 15 writers\n",
      "23/07/03 22:48:46 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 47,50% for 16 writers\n",
      "23/07/03 22:48:47 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 50,67% for 15 writers\n",
      "23/07/03 22:48:47 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 54,29% for 14 writers\n",
      "23/07/03 22:48:47 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 58,46% for 13 writers\n",
      "23/07/03 22:48:47 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 63,33% for 12 writers\n",
      "23/07/03 22:48:47 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 69,09% for 11 writers\n",
      "23/07/03 22:48:47 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 76,00% for 10 writers\n",
      "23/07/03 22:48:47 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "23/07/03 22:48:47 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_df to csv\n",
      "pandas_df to csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/03 22:49:07 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "23/07/03 22:49:07 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "23/07/03 22:49:07 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 76,00% for 10 writers\n",
      "23/07/03 22:49:07 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 69,09% for 11 writers\n",
      "23/07/03 22:49:07 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 63,33% for 12 writers\n",
      "23/07/03 22:49:07 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 58,46% for 13 writers\n",
      "23/07/03 22:49:07 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 54,29% for 14 writers\n",
      "23/07/03 22:49:07 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 50,67% for 15 writers\n",
      "23/07/03 22:49:07 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 47,50% for 16 writers\n",
      "23/07/03 22:49:08 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 50,67% for 15 writers\n",
      "23/07/03 22:49:08 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 54,29% for 14 writers\n",
      "23/07/03 22:49:08 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 58,46% for 13 writers\n",
      "23/07/03 22:49:08 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 63,33% for 12 writers\n",
      "23/07/03 22:49:08 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 69,09% for 11 writers\n",
      "23/07/03 22:49:08 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 76,00% for 10 writers\n",
      "23/07/03 22:49:08 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "23/07/03 22:49:08 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_df to csv\n",
      "(900000, 20)\n",
      "pandas_df to csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/03 22:49:28 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "23/07/03 22:49:28 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "23/07/03 22:49:28 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 76,00% for 10 writers\n",
      "23/07/03 22:49:28 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 69,09% for 11 writers\n",
      "23/07/03 22:49:28 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 63,33% for 12 writers\n",
      "23/07/03 22:49:28 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 58,46% for 13 writers\n",
      "23/07/03 22:49:28 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 54,29% for 14 writers\n",
      "23/07/03 22:49:28 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 50,67% for 15 writers\n",
      "23/07/03 22:49:28 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 47,50% for 16 writers\n",
      "23/07/03 22:49:30 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 50,67% for 15 writers\n",
      "23/07/03 22:49:30 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 54,29% for 14 writers\n",
      "23/07/03 22:49:30 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 58,46% for 13 writers\n",
      "23/07/03 22:49:30 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 63,33% for 12 writers\n",
      "23/07/03 22:49:30 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 69,09% for 11 writers\n",
      "23/07/03 22:49:30 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 76,00% for 10 writers\n",
      "23/07/03 22:49:30 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "23/07/03 22:49:30 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_df to csv\n",
      "pandas_df to csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/03 22:49:49 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "23/07/03 22:49:49 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "23/07/03 22:49:49 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 76,00% for 10 writers\n",
      "23/07/03 22:49:49 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 69,09% for 11 writers\n",
      "23/07/03 22:49:50 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 63,33% for 12 writers\n",
      "23/07/03 22:49:50 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 58,46% for 13 writers\n",
      "23/07/03 22:49:50 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 54,29% for 14 writers\n",
      "23/07/03 22:49:50 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 50,67% for 15 writers\n",
      "23/07/03 22:49:50 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 47,50% for 16 writers\n",
      "23/07/03 22:49:51 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 50,67% for 15 writers\n",
      "23/07/03 22:49:51 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 54,29% for 14 writers\n",
      "23/07/03 22:49:51 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 58,46% for 13 writers\n",
      "23/07/03 22:49:51 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 63,33% for 12 writers\n",
      "23/07/03 22:49:51 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 69,09% for 11 writers\n",
      "23/07/03 22:49:51 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 76,00% for 10 writers\n",
      "23/07/03 22:49:51 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "23/07/03 22:49:51 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_df to csv\n",
      "pandas_df to csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/03 22:50:10 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "23/07/03 22:50:10 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "23/07/03 22:50:10 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 76,00% for 10 writers\n",
      "23/07/03 22:50:10 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 69,09% for 11 writers\n",
      "23/07/03 22:50:10 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 63,33% for 12 writers\n",
      "23/07/03 22:50:10 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 58,46% for 13 writers\n",
      "23/07/03 22:50:10 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 54,29% for 14 writers\n",
      "23/07/03 22:50:10 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 50,67% for 15 writers\n",
      "23/07/03 22:50:10 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 47,50% for 16 writers\n",
      "23/07/03 22:50:12 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 50,67% for 15 writers\n",
      "23/07/03 22:50:12 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 54,29% for 14 writers\n",
      "23/07/03 22:50:12 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 58,46% for 13 writers\n",
      "23/07/03 22:50:12 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 63,33% for 12 writers\n",
      "23/07/03 22:50:12 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 69,09% for 11 writers\n",
      "23/07/03 22:50:12 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 76,00% for 10 writers\n",
      "23/07/03 22:50:12 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "23/07/03 22:50:12 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_df to csv\n",
      "pandas_df to csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/03 22:50:31 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "23/07/03 22:50:31 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "23/07/03 22:50:31 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 76,00% for 10 writers\n",
      "23/07/03 22:50:31 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 69,09% for 11 writers\n",
      "23/07/03 22:50:31 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 63,33% for 12 writers\n",
      "23/07/03 22:50:31 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 58,46% for 13 writers\n",
      "23/07/03 22:50:31 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 54,29% for 14 writers\n",
      "23/07/03 22:50:31 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 50,67% for 15 writers\n",
      "23/07/03 22:50:31 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 47,50% for 16 writers\n",
      "23/07/03 22:50:33 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 50,67% for 15 writers\n",
      "23/07/03 22:50:33 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 54,29% for 14 writers\n",
      "23/07/03 22:50:33 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 58,46% for 13 writers\n",
      "23/07/03 22:50:33 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 63,33% for 12 writers\n",
      "23/07/03 22:50:33 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 69,09% for 11 writers\n",
      "23/07/03 22:50:33 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 76,00% for 10 writers\n",
      "23/07/03 22:50:33 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "23/07/03 22:50:33 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_df to csv\n",
      "pandas_df to csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/03 22:50:53 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "23/07/03 22:50:53 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "23/07/03 22:50:53 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 76,00% for 10 writers\n",
      "23/07/03 22:50:53 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 69,09% for 11 writers\n",
      "23/07/03 22:50:53 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 63,33% for 12 writers\n",
      "23/07/03 22:50:53 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 58,46% for 13 writers\n",
      "23/07/03 22:50:53 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 54,29% for 14 writers\n",
      "23/07/03 22:50:53 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 50,67% for 15 writers\n",
      "23/07/03 22:50:53 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 47,50% for 16 writers\n",
      "23/07/03 22:50:54 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 50,67% for 15 writers\n",
      "23/07/03 22:50:54 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 54,29% for 14 writers\n",
      "23/07/03 22:50:54 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 58,46% for 13 writers\n",
      "23/07/03 22:50:54 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 63,33% for 12 writers\n",
      "23/07/03 22:50:54 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 69,09% for 11 writers\n",
      "23/07/03 22:50:54 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 76,00% for 10 writers\n",
      "23/07/03 22:50:54 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "23/07/03 22:50:54 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_df to csv\n",
      "(950000, 20)\n",
      "pandas_df to csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/03 22:51:16 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "23/07/03 22:51:16 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "23/07/03 22:51:16 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 76,00% for 10 writers\n",
      "23/07/03 22:51:16 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 69,09% for 11 writers\n",
      "23/07/03 22:51:16 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 63,33% for 12 writers\n",
      "23/07/03 22:51:16 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 58,46% for 13 writers\n",
      "23/07/03 22:51:16 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 54,29% for 14 writers\n",
      "23/07/03 22:51:16 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 50,67% for 15 writers\n",
      "23/07/03 22:51:16 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 47,50% for 16 writers\n",
      "23/07/03 22:51:17 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 50,67% for 15 writers\n",
      "23/07/03 22:51:17 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 54,29% for 14 writers\n",
      "23/07/03 22:51:17 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 58,46% for 13 writers\n",
      "23/07/03 22:51:17 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 63,33% for 12 writers\n",
      "23/07/03 22:51:17 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 69,09% for 11 writers\n",
      "23/07/03 22:51:17 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 76,00% for 10 writers\n",
      "23/07/03 22:51:17 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "23/07/03 22:51:17 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_df to csv\n",
      "pandas_df to csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/03 22:51:38 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "23/07/03 22:51:38 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "23/07/03 22:51:38 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 76,00% for 10 writers\n",
      "23/07/03 22:51:38 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 69,09% for 11 writers\n",
      "23/07/03 22:51:38 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 63,33% for 12 writers\n",
      "23/07/03 22:51:38 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 58,46% for 13 writers\n",
      "23/07/03 22:51:38 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 54,29% for 14 writers\n",
      "23/07/03 22:51:38 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 50,67% for 15 writers\n",
      "23/07/03 22:51:38 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 47,50% for 16 writers\n",
      "23/07/03 22:51:40 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 50,67% for 15 writers\n",
      "23/07/03 22:51:40 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 54,29% for 14 writers\n",
      "23/07/03 22:51:40 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 58,46% for 13 writers\n",
      "23/07/03 22:51:40 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 63,33% for 12 writers\n",
      "23/07/03 22:51:40 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 69,09% for 11 writers\n",
      "23/07/03 22:51:40 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 76,00% for 10 writers\n",
      "23/07/03 22:51:40 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "23/07/03 22:51:40 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_df to csv\n",
      "pandas_df to csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/03 22:52:01 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "23/07/03 22:52:01 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "23/07/03 22:52:01 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 76,00% for 10 writers\n",
      "23/07/03 22:52:01 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 69,09% for 11 writers\n",
      "23/07/03 22:52:01 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 63,33% for 12 writers\n",
      "23/07/03 22:52:01 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 58,46% for 13 writers\n",
      "23/07/03 22:52:01 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 54,29% for 14 writers\n",
      "23/07/03 22:52:01 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 50,67% for 15 writers\n",
      "23/07/03 22:52:01 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 47,50% for 16 writers\n",
      "23/07/03 22:52:03 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 50,67% for 15 writers\n",
      "23/07/03 22:52:03 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 54,29% for 14 writers\n",
      "23/07/03 22:52:03 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 58,46% for 13 writers\n",
      "23/07/03 22:52:03 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 63,33% for 12 writers\n",
      "23/07/03 22:52:03 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 69,09% for 11 writers\n",
      "23/07/03 22:52:03 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 76,00% for 10 writers\n",
      "23/07/03 22:52:03 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "23/07/03 22:52:03 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_df to csv\n",
      "pandas_df to csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/03 22:52:24 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "23/07/03 22:52:24 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "23/07/03 22:52:24 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 76,00% for 10 writers\n",
      "23/07/03 22:52:24 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 69,09% for 11 writers\n",
      "23/07/03 22:52:24 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 63,33% for 12 writers\n",
      "23/07/03 22:52:24 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 58,46% for 13 writers\n",
      "23/07/03 22:52:24 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 54,29% for 14 writers\n",
      "23/07/03 22:52:24 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 50,67% for 15 writers\n",
      "23/07/03 22:52:24 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 47,50% for 16 writers\n",
      "23/07/03 22:52:25 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 50,67% for 15 writers\n",
      "23/07/03 22:52:25 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 54,29% for 14 writers\n",
      "23/07/03 22:52:25 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 58,46% for 13 writers\n",
      "23/07/03 22:52:25 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 63,33% for 12 writers\n",
      "23/07/03 22:52:25 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 69,09% for 11 writers\n",
      "23/07/03 22:52:25 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 76,00% for 10 writers\n",
      "23/07/03 22:52:25 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "23/07/03 22:52:26 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_df to csv\n",
      "pandas_df to csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/03 22:52:46 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "23/07/03 22:52:46 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "23/07/03 22:52:46 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 76,00% for 10 writers\n",
      "23/07/03 22:52:46 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 69,09% for 11 writers\n",
      "23/07/03 22:52:46 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 63,33% for 12 writers\n",
      "23/07/03 22:52:46 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 58,46% for 13 writers\n",
      "23/07/03 22:52:46 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 54,29% for 14 writers\n",
      "23/07/03 22:52:46 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 50,67% for 15 writers\n",
      "23/07/03 22:52:46 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 47,50% for 16 writers\n",
      "23/07/03 22:52:48 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 50,67% for 15 writers\n",
      "23/07/03 22:52:48 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 54,29% for 14 writers\n",
      "23/07/03 22:52:48 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 58,46% for 13 writers\n",
      "23/07/03 22:52:48 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 63,33% for 12 writers\n",
      "23/07/03 22:52:48 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 69,09% for 11 writers\n",
      "23/07/03 22:52:48 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 76,00% for 10 writers\n",
      "23/07/03 22:52:48 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "23/07/03 22:52:48 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_df to csv\n",
      "(1000000, 20)\n",
      "pandas_df to csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/03 22:53:09 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "23/07/03 22:53:09 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "23/07/03 22:53:09 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 76,00% for 10 writers\n",
      "23/07/03 22:53:09 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 69,09% for 11 writers\n",
      "23/07/03 22:53:09 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 63,33% for 12 writers\n",
      "23/07/03 22:53:09 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 58,46% for 13 writers\n",
      "23/07/03 22:53:09 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 54,29% for 14 writers\n",
      "23/07/03 22:53:09 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 50,67% for 15 writers\n",
      "23/07/03 22:53:09 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 47,50% for 16 writers\n",
      "23/07/03 22:53:10 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 50,67% for 15 writers\n",
      "23/07/03 22:53:11 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 54,29% for 14 writers\n",
      "23/07/03 22:53:11 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 58,46% for 13 writers\n",
      "23/07/03 22:53:11 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 63,33% for 12 writers\n",
      "23/07/03 22:53:11 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 69,09% for 11 writers\n",
      "23/07/03 22:53:11 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 76,00% for 10 writers\n",
      "23/07/03 22:53:11 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "23/07/03 22:53:11 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_df to csv\n",
      "pandas_df to csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/03 22:53:32 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "23/07/03 22:53:32 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "23/07/03 22:53:32 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 76,00% for 10 writers\n",
      "23/07/03 22:53:32 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 69,09% for 11 writers\n",
      "23/07/03 22:53:32 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 63,33% for 12 writers\n",
      "23/07/03 22:53:32 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 58,46% for 13 writers\n",
      "23/07/03 22:53:32 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 54,29% for 14 writers\n",
      "23/07/03 22:53:32 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 50,67% for 15 writers\n",
      "23/07/03 22:53:32 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 47,50% for 16 writers\n",
      "23/07/03 22:53:33 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 50,67% for 15 writers\n",
      "23/07/03 22:53:34 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 54,29% for 14 writers\n",
      "23/07/03 22:53:34 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 58,46% for 13 writers\n",
      "23/07/03 22:53:34 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 63,33% for 12 writers\n",
      "23/07/03 22:53:34 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 69,09% for 11 writers\n",
      "23/07/03 22:53:34 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 76,00% for 10 writers\n",
      "23/07/03 22:53:34 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "23/07/03 22:53:34 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_df to csv\n",
      "pandas_df to csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/03 22:53:55 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "23/07/03 22:53:55 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "23/07/03 22:53:55 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 76,00% for 10 writers\n",
      "23/07/03 22:53:55 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 69,09% for 11 writers\n",
      "23/07/03 22:53:55 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 63,33% for 12 writers\n",
      "23/07/03 22:53:55 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 58,46% for 13 writers\n",
      "23/07/03 22:53:55 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 54,29% for 14 writers\n",
      "23/07/03 22:53:55 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 50,67% for 15 writers\n",
      "23/07/03 22:53:55 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 47,50% for 16 writers\n",
      "23/07/03 22:53:56 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 50,67% for 15 writers\n",
      "23/07/03 22:53:56 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 54,29% for 14 writers\n",
      "23/07/03 22:53:56 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 58,46% for 13 writers\n",
      "23/07/03 22:53:56 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 63,33% for 12 writers\n",
      "23/07/03 22:53:56 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 69,09% for 11 writers\n",
      "23/07/03 22:53:56 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 76,00% for 10 writers\n",
      "23/07/03 22:53:57 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "23/07/03 22:53:57 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_df to csv\n",
      "pandas_df to csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/03 22:54:17 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "23/07/03 22:54:17 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "23/07/03 22:54:17 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 76,00% for 10 writers\n",
      "23/07/03 22:54:17 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 69,09% for 11 writers\n",
      "23/07/03 22:54:17 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 63,33% for 12 writers\n",
      "23/07/03 22:54:17 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 58,46% for 13 writers\n",
      "23/07/03 22:54:17 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 54,29% for 14 writers\n",
      "23/07/03 22:54:17 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 50,67% for 15 writers\n",
      "23/07/03 22:54:17 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 47,50% for 16 writers\n",
      "23/07/03 22:54:19 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 50,67% for 15 writers\n",
      "23/07/03 22:54:19 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 54,29% for 14 writers\n",
      "23/07/03 22:54:19 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 58,46% for 13 writers\n",
      "23/07/03 22:54:19 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 63,33% for 12 writers\n",
      "23/07/03 22:54:19 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 69,09% for 11 writers\n",
      "23/07/03 22:54:19 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 76,00% for 10 writers\n",
      "23/07/03 22:54:19 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "23/07/03 22:54:19 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_df to csv\n",
      "pandas_df to csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/03 22:54:40 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "23/07/03 22:54:40 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "23/07/03 22:54:40 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 76,00% for 10 writers\n",
      "23/07/03 22:54:40 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 69,09% for 11 writers\n",
      "23/07/03 22:54:40 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 63,33% for 12 writers\n",
      "23/07/03 22:54:40 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 58,46% for 13 writers\n",
      "23/07/03 22:54:40 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 54,29% for 14 writers\n",
      "23/07/03 22:54:40 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 50,67% for 15 writers\n",
      "23/07/03 22:54:40 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 47,50% for 16 writers\n",
      "23/07/03 22:54:42 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 50,67% for 15 writers\n",
      "[Stage 59:===>                                                    (1 + 15) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_df to csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/03 22:54:42 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 54,29% for 14 writers\n",
      "23/07/03 22:54:42 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 58,46% for 13 writers\n",
      "23/07/03 22:54:42 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 63,33% for 12 writers\n",
      "23/07/03 22:54:42 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 69,09% for 11 writers\n",
      "23/07/03 22:54:42 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 76,00% for 10 writers\n",
      "23/07/03 22:54:42 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "23/07/03 22:54:42 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# load time statistics\n",
    "time_statistics_large = pd.read_csv(\"data/time_statistics_large.csv\")\n",
    "\n",
    "# I iterated in steps - because of memory allocation issues\n",
    "for i in range(0, 3):\n",
    "    df_pd = large_step_pd[i]\n",
    "    df_spark = large_step_spark[i]\n",
    "    print(df_pd.shape)\n",
    "    average_pandas_time, average_pyspark_time, _ , _ = utils.iterations(test.write_data, 5, spark, df_pd, df_spark)\n",
    "    time_statistics_large.loc[(time_statistics_large['row_count'] == df_pd.shape[0]) & (time_statistics_large[\"column_count\"] == df_pd.shape[1]), [\"write_pd\", \"write_spark\"]] = [average_pandas_time, average_pyspark_time]\n",
    "\n",
    "# save time statistics\n",
    "time_statistics_large.to_csv(\"data/time_statistics_large.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 2: Load Dataframe from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 20)\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "(20000, 20)\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "(30000, 20)\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "(40000, 20)\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "(50000, 20)\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "(60000, 20)\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "(70000, 20)\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "(80000, 20)\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "(90000, 20)\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "(100000, 20)\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n"
     ]
    }
   ],
   "source": [
    "# load time small statistics\n",
    "time_statistics_small = pd.read_csv(\"data/time_statistics_small.csv\")\n",
    "\n",
    "# iterate over small step dataframes\n",
    "for i in range(len(small_step_pd)):\n",
    "    df_pd = small_step_pd[i]\n",
    "    df_spark = small_step_spark[i]\n",
    "    print(df_pd.shape)\n",
    "    average_pandas_time, average_pyspark_time, _ , _ = utils.iterations(test.load_data, 5, spark, df_pd, df_spark)\n",
    "    time_statistics_small.loc[(time_statistics_small['row_count'] == df_pd.shape[0]) & (time_statistics_small[\"column_count\"] == df_pd.shape[1]), [\"read_pd\", \"read_spark\"]] = [average_pandas_time, average_pyspark_time]\n",
    "\n",
    "# save time small statistics\n",
    "time_statistics_small.to_csv(\"data/time_statistics_small.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 20)\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "(100000, 20)\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "(150000, 20)\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "(200000, 20)\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "(250000, 20)\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "(300000, 20)\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "(350000, 20)\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "(400000, 20)\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "(450000, 20)\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_df to csv\n",
      "pandas_df to csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_df to csv\n",
      "(500000, 20)\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_df to csv\n",
      "(550000, 20)\n",
      "pandas_df to csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_df to csv\n",
      "pandas_df to csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_df to csv\n",
      "pandas_df to csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "(600000, 20)\n",
      "pandas_df to csv\n",
      "spark_df to csv\n",
      "pandas_df to csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_df to csv\n",
      "pandas_df to csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_df to csv\n",
      "pandas_df to csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_df to csv\n",
      "pandas_df to csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_df to csv\n",
      "(650000, 20)\n",
      "pandas_df to csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_df to csv\n",
      "pandas_df to csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_df to csv\n",
      "pandas_df to csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_df to csv\n",
      "pandas_df to csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_df to csv\n",
      "pandas_df to csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_df to csv\n",
      "(700000, 20)\n",
      "pandas_df to csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_df to csv\n",
      "pandas_df to csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_df to csv\n",
      "pandas_df to csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_df to csv\n",
      "pandas_df to csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_df to csv\n",
      "pandas_df to csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_df to csv\n",
      "(750000, 20)\n",
      "pandas_df to csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_df to csv\n",
      "pandas_df to csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_df to csv\n",
      "pandas_df to csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_df to csv\n",
      "pandas_df to csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_df to csv\n",
      "pandas_df to csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_df to csv\n",
      "(800000, 20)\n",
      "pandas_df to csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_df to csv\n",
      "pandas_df to csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_df to csv\n",
      "pandas_df to csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_df to csv\n",
      "pandas_df to csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_df to csv\n",
      "pandas_df to csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_df to csv\n",
      "(850000, 20)\n",
      "pandas_df to csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_df to csv\n",
      "pandas_df to csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_df to csv\n",
      "pandas_df to csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_df to csv\n",
      "pandas_df to csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_df to csv\n",
      "pandas_df to csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_df to csv\n",
      "(900000, 20)\n",
      "pandas_df to csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_df to csv\n",
      "pandas_df to csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_df to csv\n",
      "pandas_df to csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_df to csv\n",
      "pandas_df to csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_df to csv\n",
      "pandas_df to csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_df to csv\n",
      "(950000, 20)\n",
      "pandas_df to csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_df to csv\n",
      "pandas_df to csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_df to csv\n",
      "pandas_df to csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_df to csv\n",
      "pandas_df to csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_df to csv\n",
      "pandas_df to csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_df to csv\n",
      "(1000000, 20)\n",
      "pandas_df to csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_df to csv\n",
      "pandas_df to csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_df to csv\n",
      "pandas_df to csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_df to csv\n",
      "pandas_df to csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_df to csv\n",
      "pandas_df to csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 361:>                                                      (0 + 16) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_df to csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# load time statistics\n",
    "time_statistics_large = pd.read_csv(\"data/time_statistics_large.csv\")\n",
    "\n",
    "# I iterated in steps - because of memory allocation issues\n",
    "for i in range(len(large_step_pd)):\n",
    "    df_pd = large_step_pd[i]\n",
    "    df_spark = large_step_spark[i]\n",
    "    print(df_pd.shape)\n",
    "    average_pandas_time, average_pyspark_time, _ , _ = utils.iterations(test.load_data, 5, spark, df_pd, df_spark)\n",
    "    time_statistics_large.loc[(time_statistics_large['row_count'] == df_pd.shape[0]) & (time_statistics_large[\"column_count\"] == df_pd.shape[1]), [\"read_pd\", \"read_spark\"]] = [average_pandas_time, average_pyspark_time]\n",
    "\n",
    "# save time statistics\n",
    "time_statistics_large.to_csv(\"data/time_statistics_large.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 3: Drop NaN Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 20)\n",
      "(20000, 20)\n",
      "(30000, 20)\n",
      "(40000, 20)\n",
      "(50000, 20)\n",
      "(60000, 20)\n",
      "(70000, 20)\n",
      "(80000, 20)\n",
      "(90000, 20)\n",
      "(100000, 20)\n"
     ]
    }
   ],
   "source": [
    "# load time small statistics\n",
    "time_statistics_small = pd.read_csv(\"data/time_statistics_small.csv\")\n",
    "\n",
    "# run test\n",
    "time_statistics_small = utils.test_run(test.drop_nan, \"drop_na_pd\", \"drop_na_spark\", time_statistics_small, small_step_pd, small_step_spark, spark, 100)\n",
    "\n",
    "# save time small statistics\n",
    "time_statistics_small.to_csv(\"data/time_statistics_small.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 20)\n",
      "(100000, 20)\n",
      "(150000, 20)\n",
      "(200000, 20)\n",
      "(250000, 20)\n",
      "(300000, 20)\n",
      "(350000, 20)\n",
      "(400000, 20)\n",
      "(450000, 20)\n",
      "(500000, 20)\n",
      "(550000, 20)\n",
      "(600000, 20)\n",
      "(650000, 20)\n",
      "(700000, 20)\n",
      "(750000, 20)\n",
      "(800000, 20)\n",
      "(850000, 20)\n",
      "(900000, 20)\n",
      "(950000, 20)\n",
      "(1000000, 20)\n"
     ]
    }
   ],
   "source": [
    "# load time statistics\n",
    "time_statistics_large = pd.read_csv(\"data/time_statistics_large.csv\")\n",
    "\n",
    "# run test\n",
    "time_statistics_large = utils.test_run(test.drop_nan, \"drop_na_pd\", \"drop_na_spark\", time_statistics_large, large_step_pd, large_step_spark, spark, 100)\n",
    "\n",
    "# save time statistics\n",
    "time_statistics_large.to_csv(\"data/time_statistics_large.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 4: Fill NaN Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 20)\n",
      "(20000, 20)\n",
      "(30000, 20)\n",
      "(40000, 20)\n",
      "(50000, 20)\n",
      "(60000, 20)\n",
      "(70000, 20)\n",
      "(80000, 20)\n",
      "(90000, 20)\n",
      "(100000, 20)\n"
     ]
    }
   ],
   "source": [
    "# load time small statistics\n",
    "time_statistics_small = pd.read_csv(\"data/time_statistics_small.csv\")\n",
    "\n",
    "# run test\n",
    "time_statistics_small = utils.test_run(test.fill_nan, \"fill_na_pd\", \"fill_na_spark\", time_statistics_small, small_step_pd, small_step_spark, spark, 100)\n",
    "\n",
    "# save time small statistics\n",
    "time_statistics_small.to_csv(\"data/time_statistics_small.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 20)\n",
      "(100000, 20)\n",
      "(150000, 20)\n",
      "(200000, 20)\n",
      "(250000, 20)\n",
      "(300000, 20)\n",
      "(350000, 20)\n",
      "(400000, 20)\n",
      "(450000, 20)\n",
      "(500000, 20)\n",
      "(550000, 20)\n",
      "(600000, 20)\n",
      "(650000, 20)\n",
      "(700000, 20)\n",
      "(750000, 20)\n",
      "(800000, 20)\n",
      "(850000, 20)\n",
      "(900000, 20)\n",
      "(950000, 20)\n",
      "(1000000, 20)\n"
     ]
    }
   ],
   "source": [
    "# load time statistics\n",
    "time_statistics_large = pd.read_csv(\"data/time_statistics_large.csv\")\n",
    "\n",
    "# run test\n",
    "time_statistics_large = utils.test_run(test.fill_nan, \"fill_na_pd\", \"fill_na_spark\", time_statistics_large, large_step_pd, large_step_spark, spark, 50)\n",
    "\n",
    "# save time statistics\n",
    "time_statistics_large.to_csv(\"data/time_statistics_large.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 5: Groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 20)\n",
      "(20000, 20)\n",
      "(30000, 20)\n",
      "(40000, 20)\n",
      "(50000, 20)\n",
      "(60000, 20)\n",
      "(70000, 20)\n",
      "(80000, 20)\n",
      "(90000, 20)\n",
      "(100000, 20)\n"
     ]
    }
   ],
   "source": [
    "# load time small statistics\n",
    "time_statistics_small = pd.read_csv(\"data/time_statistics_small.csv\")\n",
    "\n",
    "# run test\n",
    "time_statistics_small = utils.test_run(test.group_df, \"group_pd\", \"group_spark\", time_statistics_small, small_step_pd, small_step_spark, spark, 100)\n",
    "\n",
    "# save time small statistics\n",
    "time_statistics_small.to_csv(\"data/time_statistics_small.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 20)\n",
      "(100000, 20)\n",
      "(150000, 20)\n",
      "(200000, 20)\n",
      "(250000, 20)\n",
      "(300000, 20)\n",
      "(350000, 20)\n",
      "(400000, 20)\n",
      "(450000, 20)\n",
      "(500000, 20)\n",
      "(550000, 20)\n",
      "(600000, 20)\n",
      "(650000, 20)\n",
      "(700000, 20)\n",
      "(750000, 20)\n",
      "(800000, 20)\n",
      "(850000, 20)\n",
      "(900000, 20)\n",
      "(950000, 20)\n",
      "(1000000, 20)\n"
     ]
    }
   ],
   "source": [
    "# load time statistics\n",
    "time_statistics_large = pd.read_csv(\"data/time_statistics_large.csv\")\n",
    "\n",
    "# run test\n",
    "time_statistics_large = utils.test_run(test.group_df, \"group_pd\", \"group_spark\", time_statistics_large, large_step_pd, large_step_spark, spark, 100)\n",
    "\n",
    "# save time statistics\n",
    "time_statistics_large.to_csv(\"data/time_statistics_large.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 6: GroupBy and Sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 20)\n",
      "(20000, 20)\n",
      "(30000, 20)\n",
      "(40000, 20)\n",
      "(50000, 20)\n",
      "(60000, 20)\n",
      "(70000, 20)\n",
      "(80000, 20)\n",
      "(90000, 20)\n",
      "(100000, 20)\n"
     ]
    }
   ],
   "source": [
    "# load time small statistics\n",
    "time_statistics_small = pd.read_csv(\"data/time_statistics_small.csv\")\n",
    "\n",
    "# run test\n",
    "time_statistics_small = utils.test_run(test.group_sum_df, \"group_sum_pd\", \"group_sum_spark\", time_statistics_small, small_step_pd, small_step_spark, spark, 50)\n",
    "\n",
    "# save time small statistics\n",
    "time_statistics_small.to_csv(\"data/time_statistics_small.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 20)\n",
      "(100000, 20)\n",
      "(150000, 20)\n",
      "(200000, 20)\n",
      "(250000, 20)\n",
      "(300000, 20)\n",
      "(350000, 20)\n",
      "(400000, 20)\n",
      "(450000, 20)\n",
      "(500000, 20)\n",
      "(550000, 20)\n",
      "(600000, 20)\n",
      "(650000, 20)\n",
      "(700000, 20)\n",
      "(750000, 20)\n",
      "(800000, 20)\n",
      "(850000, 20)\n",
      "(900000, 20)\n",
      "(950000, 20)\n",
      "(1000000, 20)\n"
     ]
    }
   ],
   "source": [
    "# load time statistics\n",
    "time_statistics_large = pd.read_csv(\"data/time_statistics_large.csv\")\n",
    "\n",
    "# run test\n",
    "time_statistics_large = utils.test_run(test.group_sum_df, \"group_sum_pd\", \"group_sum_spark\", time_statistics_large, large_step_pd, large_step_spark, spark, 30)\n",
    "\n",
    "# save time statistics\n",
    "time_statistics_large.to_csv(\"data/time_statistics_large.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 7: GroupBy and Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 20)\n",
      "(20000, 20)\n",
      "(30000, 20)\n",
      "(40000, 20)\n",
      "(50000, 20)\n",
      "(60000, 20)\n",
      "(70000, 20)\n",
      "(80000, 20)\n",
      "(90000, 20)\n",
      "(100000, 20)\n"
     ]
    }
   ],
   "source": [
    "# load time small statistics\n",
    "time_statistics_small = pd.read_csv(\"data/time_statistics_small.csv\")\n",
    "\n",
    "# run test\n",
    "time_statistics_small = utils.test_run(test.group_count_df, \"group_count_pd\", \"group_count_spark\", time_statistics_small, small_step_pd, small_step_spark, spark, 50)\n",
    "\n",
    "# save time small statistics\n",
    "time_statistics_small.to_csv(\"data/time_statistics_small.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 20)\n",
      "(100000, 20)\n",
      "(150000, 20)\n",
      "(200000, 20)\n",
      "(250000, 20)\n",
      "(300000, 20)\n",
      "(350000, 20)\n",
      "(400000, 20)\n",
      "(450000, 20)\n",
      "(500000, 20)\n",
      "(550000, 20)\n",
      "(600000, 20)\n",
      "(650000, 20)\n",
      "(700000, 20)\n",
      "(750000, 20)\n",
      "(800000, 20)\n",
      "(850000, 20)\n",
      "(900000, 20)\n",
      "(950000, 20)\n",
      "(1000000, 20)\n"
     ]
    }
   ],
   "source": [
    "# load time statistics\n",
    "time_statistics_large = pd.read_csv(\"data/time_statistics_large.csv\")\n",
    "\n",
    "# run test\n",
    "time_statistics_large = utils.test_run(test.group_count_df, \"group_count_pd\", \"group_count_spark\", time_statistics_large, large_step_pd, large_step_spark, spark, 30)\n",
    "\n",
    "# save time statistics\n",
    "time_statistics_large.to_csv(\"data/time_statistics_large.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 8: Filter by Column Value (under 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 20)\n",
      "(20000, 20)\n",
      "(30000, 20)\n",
      "(40000, 20)\n",
      "(50000, 20)\n",
      "(60000, 20)\n",
      "(70000, 20)\n",
      "(80000, 20)\n",
      "(90000, 20)\n",
      "(100000, 20)\n"
     ]
    }
   ],
   "source": [
    "# load time small statistics\n",
    "time_statistics_small = pd.read_csv(\"data/time_statistics_small.csv\")\n",
    "\n",
    "# run test\n",
    "time_statistics_small = utils.test_run(test.filter_less_0, \"filter_less_0_pd\", \"filter_less_0_spark\", time_statistics_small, small_step_pd, small_step_spark, spark, 100)\n",
    "\n",
    "# save time small statistics\n",
    "time_statistics_small.to_csv(\"data/time_statistics_small.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 20)\n",
      "(100000, 20)\n",
      "(150000, 20)\n",
      "(200000, 20)\n",
      "(250000, 20)\n",
      "(300000, 20)\n",
      "(350000, 20)\n",
      "(400000, 20)\n",
      "(450000, 20)\n",
      "(500000, 20)\n",
      "(550000, 20)\n",
      "(600000, 20)\n",
      "(650000, 20)\n",
      "(700000, 20)\n",
      "(750000, 20)\n",
      "(800000, 20)\n",
      "(850000, 20)\n",
      "(900000, 20)\n",
      "(950000, 20)\n",
      "(1000000, 20)\n"
     ]
    }
   ],
   "source": [
    "# load time statistics\n",
    "time_statistics_large = pd.read_csv(\"data/time_statistics_large.csv\")\n",
    "\n",
    "# run test\n",
    "time_statistics_large = utils.test_run(test.filter_less_0, \"filter_less_0_pd\", \"filter_less_0_spark\", time_statistics_large, large_step_pd, large_step_spark, spark, 50)\n",
    "\n",
    "# save time statistics\n",
    "time_statistics_large.to_csv(\"data/time_statistics_large.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 9: Filter by Column Value (under 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 20)\n",
      "(20000, 20)\n",
      "(30000, 20)\n",
      "(40000, 20)\n",
      "(50000, 20)\n",
      "(60000, 20)\n",
      "(70000, 20)\n",
      "(80000, 20)\n",
      "(90000, 20)\n",
      "(100000, 20)\n"
     ]
    }
   ],
   "source": [
    "# load time small statistics\n",
    "time_statistics_small = pd.read_csv(\"data/time_statistics_small.csv\")\n",
    "\n",
    "# run test\n",
    "time_statistics_small = utils.test_run(test.filter_less_10, \"filter_less_10_pd\", \"filter_less_10_spark\", time_statistics_small, small_step_pd, small_step_spark, spark, 100)\n",
    "\n",
    "# save time small statistics\n",
    "time_statistics_small.to_csv(\"data/time_statistics_small.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 20)\n",
      "(100000, 20)\n",
      "(150000, 20)\n",
      "(200000, 20)\n",
      "(250000, 20)\n",
      "(300000, 20)\n",
      "(350000, 20)\n",
      "(400000, 20)\n",
      "(450000, 20)\n",
      "(500000, 20)\n",
      "(550000, 20)\n",
      "(600000, 20)\n",
      "(650000, 20)\n",
      "(700000, 20)\n",
      "(750000, 20)\n",
      "(800000, 20)\n",
      "(850000, 20)\n",
      "(900000, 20)\n",
      "(950000, 20)\n",
      "(1000000, 20)\n"
     ]
    }
   ],
   "source": [
    "# load time statistics\n",
    "time_statistics_large = pd.read_csv(\"data/time_statistics_large.csv\")\n",
    "\n",
    "# run test\n",
    "time_statistics_large = utils.test_run(test.filter_less_10, \"filter_less_10_pd\", \"filter_less_10_spark\", time_statistics_large, large_step_pd, large_step_spark, spark, 50)\n",
    "\n",
    "# save time statistics\n",
    "time_statistics_large.to_csv(\"data/time_statistics_large.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 10: Join Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 20)\n",
      "(20000, 20)\n",
      "(30000, 20)\n",
      "(40000, 20)\n",
      "(50000, 20)\n"
     ]
    }
   ],
   "source": [
    "# load time small statistics\n",
    "time_statistics_small = pd.read_csv(\"data/time_statistics_small.csv\")\n",
    "\n",
    "# run test\n",
    "time_statistics_small = utils.test_run(test.join_df, \"join_pd\", \"join_spark\", time_statistics_small, small_step_pd, small_step_spark, spark, 5)\n",
    "\n",
    "# save time small statistics\n",
    "time_statistics_small.to_csv(\"data/time_statistics_small.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load time statistics\n",
    "time_statistics_large = pd.read_csv(\"data/time_statistics_large.csv\")\n",
    "\n",
    "# run test\n",
    "time_statistics_large = utils.test_run(test.join_df, \"join_pd\", \"join_spark\", time_statistics_large, large_step_pd, large_step_spark, spark, 3)\n",
    "\n",
    "# save time statistics\n",
    "time_statistics_large.to_csv(\"data/time_statistics_large.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 11: Multiplication (Build-In)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 20)\n",
      "(20000, 20)\n",
      "(30000, 20)\n",
      "(40000, 20)\n",
      "(50000, 20)\n",
      "(60000, 20)\n",
      "(70000, 20)\n",
      "(80000, 20)\n",
      "(90000, 20)\n",
      "(100000, 20)\n"
     ]
    }
   ],
   "source": [
    "# load time statistics\n",
    "time_statistics_small = pd.read_csv(\"data/time_statistics_small.csv\")\n",
    "\n",
    "# run test\n",
    "time_statistics_small = utils.test_run(test.multiply_build_in, \"mul_build_pd\", \"mul_build_spark\", time_statistics_small, small_step_pd, small_step_spark, 100000)\n",
    "\n",
    "# save time small statistics\n",
    "time_statistics_small.to_csv(\"data/time_statistics_small.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 20)\n",
      "(100000, 20)\n",
      "(150000, 20)\n",
      "(200000, 20)\n",
      "(250000, 20)\n",
      "(300000, 20)\n",
      "(350000, 20)\n",
      "(400000, 20)\n",
      "(450000, 20)\n",
      "(500000, 20)\n",
      "(550000, 20)\n",
      "(600000, 20)\n",
      "(650000, 20)\n",
      "(700000, 20)\n",
      "(750000, 20)\n",
      "(800000, 20)\n",
      "(850000, 20)\n",
      "(900000, 20)\n",
      "(950000, 20)\n",
      "(1000000, 20)\n"
     ]
    }
   ],
   "source": [
    "# load time statistics\n",
    "time_statistics_large = pd.read_csv(\"data/time_statistics_large.csv\")\n",
    "\n",
    "# run test\n",
    "time_statistics_large = utils.test_run(test.multiply_build_in, \"mul_build_pd\", \"mul_build_spark\", time_statistics_large, large_step_pd, large_step_spark, 100000)\n",
    "\n",
    "# save time statistics\n",
    "time_statistics_large.to_csv(\"data/time_statistics_large.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 12: Multiplication (Column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 20)\n",
      "(20000, 20)\n",
      "(30000, 20)\n",
      "(40000, 20)\n",
      "(50000, 20)\n",
      "(60000, 20)\n",
      "(70000, 20)\n",
      "(80000, 20)\n",
      "(90000, 20)\n",
      "(100000, 20)\n"
     ]
    }
   ],
   "source": [
    "# load time small statistics\n",
    "time_statistics_small = pd.read_csv(\"data/time_statistics_small.csv\")\n",
    "\n",
    "# run test\n",
    "time_statistics_small = utils.test_run(test.multiply_by_selection, \"mul_col_pd\", \"mul_col_spark\", time_statistics_small, small_step_pd, small_step_spark, spark, 1000)\n",
    "\n",
    "# save time small statistics\n",
    "time_statistics_small.to_csv(\"data/time_statistics_small.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 20)\n",
      "(100000, 20)\n",
      "(150000, 20)\n",
      "(200000, 20)\n",
      "(250000, 20)\n",
      "(300000, 20)\n",
      "(350000, 20)\n",
      "(400000, 20)\n",
      "(450000, 20)\n",
      "(500000, 20)\n",
      "(550000, 20)\n",
      "(600000, 20)\n",
      "(650000, 20)\n",
      "(700000, 20)\n",
      "(750000, 20)\n",
      "(800000, 20)\n",
      "(850000, 20)\n",
      "(900000, 20)\n",
      "(950000, 20)\n",
      "(1000000, 20)\n"
     ]
    }
   ],
   "source": [
    "# load time statistics\n",
    "time_statistics_large = pd.read_csv(\"data/time_statistics_large.csv\")\n",
    "\n",
    "# run test\n",
    "time_statistics_large = utils.test_run(test.multiply_by_selection, \"mul_col_pd\", \"mul_col_spark\", time_statistics_large, large_step_pd, large_step_spark, spark, 500)\n",
    "\n",
    "# save time statistics\n",
    "time_statistics_large.to_csv(\"data/time_statistics_large.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 13: Convert Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 20)\n",
      "(20000, 20)\n",
      "(30000, 20)\n",
      "(40000, 20)\n",
      "(50000, 20)\n",
      "(60000, 20)\n",
      "(70000, 20)\n",
      "(80000, 20)\n",
      "(90000, 20)\n",
      "(100000, 20)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/9w/qmy847jn773gkcprxl_lm43h0000gn/T/ipykernel_52690/3750949356.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# run test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtime_statistics_small\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pd_to_pyspark\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pyspark_to_pd\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_statistics_small\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmall_step_pd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmall_step_spark\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# save time small statistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repository/pandas-pyspark-analysis_medium/utils.py\u001b[0m in \u001b[0;36mtest_run\u001b[0;34m(test_func, pandas_column_name, spark_column_name, statistics_df, dataframes_pd, dataframes_spark, spark_session, iterations)\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0mdf_spark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataframes_spark\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_pd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0maverage_pandas_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage_pyspark_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_iterations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspark_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_pd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_spark\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         \u001b[0mstatistics_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatistics_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'row_count'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdf_pd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstatistics_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"column_count\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdf_pd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpandas_column_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspark_column_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0maverage_pandas_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage_pyspark_time\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repository/pandas-pyspark-analysis_medium/utils.py\u001b[0m in \u001b[0;36mtest_iterations\u001b[0;34m(test_func, iterations, spark_session, df_pd, df_spark)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m# execute test function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mpandas_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpyspark_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_pd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_spark\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspark_session\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mpandas_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpandas_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mpyspark_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyspark_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repository/pandas-pyspark-analysis_medium/test_functions.py\u001b[0m in \u001b[0;36mconvert_df\u001b[0;34m(df_pd, df_spark, spark_session)\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0mpd_counter_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter_ns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m     \u001b[0;31m# Pandas to PySpark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_pd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[0;31m### end pandas timer ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/PySpark_comp/lib/python3.7/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    672\u001b[0m             \u001b[0;31m# Create a DataFrame from pandas DataFrame.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m             return super(SparkSession, self).createDataFrame(\n\u001b[0;32m--> 674\u001b[0;31m                 data, schema, samplingRatio, verifySchema)\n\u001b[0m\u001b[1;32m    675\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/PySpark_comp/lib/python3.7/site-packages/pyspark/sql/pandas/conversion.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    338\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_from_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimezone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_convert_from_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimezone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/PySpark_comp/lib/python3.7/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    698\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 700\u001b[0;31m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    701\u001b[0m         \u001b[0mjrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerDeUtil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoJavaArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_java_object_rdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplySchemaToPythonRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/PySpark_comp/lib/python3.7/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m             \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inferSchemaFromList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    513\u001b[0m             \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_converter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/PySpark_comp/lib/python3.7/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_inferSchemaFromList\u001b[0;34m(self, data, names)\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"can not infer schema from empty dataset\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m         \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_merge_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_infer_schema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_has_nulltype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Some of types cannot be determined after inferring\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/PySpark_comp/lib/python3.7/site-packages/pyspark/sql/types.py\u001b[0m in \u001b[0;36m_merge_type\u001b[0;34m(a, b, name)\u001b[0m\n\u001b[1;32m   1106\u001b[0m     \u001b[0;31m# same type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStructType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1108\u001b[0;31m         \u001b[0mnfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataType\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1109\u001b[0m         fields = [StructField(f.name, _merge_type(f.dataType, nfs.get(f.name, NullType()),\n\u001b[1;32m   1110\u001b[0m                                                   name=new_name(f.name)))\n",
      "\u001b[0;32m~/opt/anaconda3/envs/PySpark_comp/lib/python3.7/site-packages/pyspark/sql/types.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1106\u001b[0m     \u001b[0;31m# same type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStructType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1108\u001b[0;31m         \u001b[0mnfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataType\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1109\u001b[0m         fields = [StructField(f.name, _merge_type(f.dataType, nfs.get(f.name, NullType()),\n\u001b[1;32m   1110\u001b[0m                                                   name=new_name(f.name)))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# load time small statistics\n",
    "time_statistics_small = pd.read_csv(\"data/time_statistics_small.csv\")\n",
    "\n",
    "# run test\n",
    "time_statistics_small = utils.test_run(test.convert_df, \"pd_to_spark\", \"pyspark_to_pd\", time_statistics_small, small_step_pd, small_step_spark, spark, 5)\n",
    "\n",
    "# save time small statistics\n",
    "time_statistics_small.to_csv(\"data/time_statistics_small.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 20)\n",
      "(100000, 20)\n",
      "(150000, 20)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/9w/qmy847jn773gkcprxl_lm43h0000gn/T/ipykernel_52690/2778411976.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# run test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtime_statistics_large\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pd_to_spark\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pyspark_to_pd\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_statistics_large\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlarge_step_pd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlarge_step_spark\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# save time statistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repository/pandas-pyspark-analysis_medium/utils.py\u001b[0m in \u001b[0;36mtest_run\u001b[0;34m(test_func, pandas_column_name, spark_column_name, statistics_df, dataframes_pd, dataframes_spark, spark_session, iterations)\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0mdf_spark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataframes_spark\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_pd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0maverage_pandas_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage_pyspark_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_iterations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspark_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_pd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_spark\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         \u001b[0mstatistics_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatistics_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'row_count'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdf_pd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstatistics_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"column_count\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdf_pd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpandas_column_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspark_column_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0maverage_pandas_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage_pyspark_time\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repository/pandas-pyspark-analysis_medium/utils.py\u001b[0m in \u001b[0;36mtest_iterations\u001b[0;34m(test_func, iterations, spark_session, df_pd, df_spark)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m# execute test function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mpandas_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpyspark_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_pd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_spark\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspark_session\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mpandas_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpandas_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mpyspark_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyspark_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repository/pandas-pyspark-analysis_medium/test_functions.py\u001b[0m in \u001b[0;36mconvert_df\u001b[0;34m(df_pd, df_spark, spark_session)\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0mpd_counter_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter_ns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m     \u001b[0;31m# Pandas to PySpark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_pd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[0;31m### end pandas timer ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/PySpark_comp/lib/python3.7/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    672\u001b[0m             \u001b[0;31m# Create a DataFrame from pandas DataFrame.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m             return super(SparkSession, self).createDataFrame(\n\u001b[0;32m--> 674\u001b[0;31m                 data, schema, samplingRatio, verifySchema)\n\u001b[0m\u001b[1;32m    675\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/PySpark_comp/lib/python3.7/site-packages/pyspark/sql/pandas/conversion.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    338\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_from_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimezone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_convert_from_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimezone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/PySpark_comp/lib/python3.7/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    698\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 700\u001b[0;31m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    701\u001b[0m         \u001b[0mjrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerDeUtil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoJavaArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_java_object_rdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplySchemaToPythonRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/PySpark_comp/lib/python3.7/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m             \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inferSchemaFromList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    513\u001b[0m             \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_converter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/PySpark_comp/lib/python3.7/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_inferSchemaFromList\u001b[0;34m(self, data, names)\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"can not infer schema from empty dataset\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m         \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_merge_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_infer_schema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_has_nulltype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Some of types cannot be determined after inferring\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/PySpark_comp/lib/python3.7/site-packages/pyspark/sql/types.py\u001b[0m in \u001b[0;36m_merge_type\u001b[0;34m(a, b, name)\u001b[0m\n\u001b[1;32m   1109\u001b[0m         fields = [StructField(f.name, _merge_type(f.dataType, nfs.get(f.name, NullType()),\n\u001b[1;32m   1110\u001b[0m                                                   name=new_name(f.name)))\n\u001b[0;32m-> 1111\u001b[0;31m                   for f in a.fields]\n\u001b[0m\u001b[1;32m   1112\u001b[0m         \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnfs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/PySpark_comp/lib/python3.7/site-packages/pyspark/sql/types.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1109\u001b[0m         fields = [StructField(f.name, _merge_type(f.dataType, nfs.get(f.name, NullType()),\n\u001b[1;32m   1110\u001b[0m                                                   name=new_name(f.name)))\n\u001b[0;32m-> 1111\u001b[0;31m                   for f in a.fields]\n\u001b[0m\u001b[1;32m   1112\u001b[0m         \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnfs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/PySpark_comp/lib/python3.7/site-packages/pyspark/sql/types.py\u001b[0m in \u001b[0;36m_merge_type\u001b[0;34m(a, b, name)\u001b[0m\n\u001b[1;32m   1100\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNullType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1102\u001b[0;31m     \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# TODO: type cast (such as int -> long)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Can not merge type %s and %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# load time statistics\n",
    "time_statistics_large = pd.read_csv(\"data/time_statistics_large.csv\")\n",
    "\n",
    "# run test\n",
    "time_statistics_large = utils.test_run(test.convert_df, \"pd_to_spark\", \"pyspark_to_pd\", time_statistics_large, large_step_pd, large_step_spark, spark, 1)\n",
    "\n",
    "# save time statistics\n",
    "time_statistics_large.to_csv(\"data/time_statistics_large.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HELPER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.stop()\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Performance Analysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# # load small step data\n",
    "# small_step_spark = []\n",
    "# for i in range(10_000, 100_001, 10_000):\n",
    "#     df = spark.read.csv(f\"data/small/pandas_test_{i}_rows.csv\", header=True, inferSchema=True, sep=\",\")\n",
    "#     small_step_spark.append(df)\n",
    "\n",
    "# load large step data\n",
    "large_step_spark = []\n",
    "for i in range(50_000, 1_000_001, 50_000):\n",
    "    df = spark.read.csv(f\"data/large/pandas_test_{i}_rows.csv\", header=True, inferSchema=True)\n",
    "    large_step_spark.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_statistics_small = pd.read_csv(\"data/time_statistics_small.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop column write_pd\n",
    "time_statistics_small = time_statistics_small.drop(columns=[\"write_pd\", \"write_spark\", \"read_pd\", \"read_spark\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_count</th>\n",
       "      <th>column_count</th>\n",
       "      <th>drop_na_pd</th>\n",
       "      <th>drop_na_spark</th>\n",
       "      <th>fill_na_pd</th>\n",
       "      <th>fill_na_spark</th>\n",
       "      <th>group_pd</th>\n",
       "      <th>group_spark</th>\n",
       "      <th>group_sum_pd</th>\n",
       "      <th>group_sum_spark</th>\n",
       "      <th>...</th>\n",
       "      <th>filter_less_10_pd</th>\n",
       "      <th>filter_less_10_spark</th>\n",
       "      <th>join_pd</th>\n",
       "      <th>join_spark</th>\n",
       "      <th>mul_build_pd</th>\n",
       "      <th>mul_build_spark</th>\n",
       "      <th>mul_col_pd</th>\n",
       "      <th>mul_col_spark</th>\n",
       "      <th>pd_to_pyspark</th>\n",
       "      <th>pyspark_to_pd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000</td>\n",
       "      <td>20</td>\n",
       "      <td>2273045.35</td>\n",
       "      <td>6127367.31</td>\n",
       "      <td>930211.45</td>\n",
       "      <td>11401758.28</td>\n",
       "      <td>100231.27</td>\n",
       "      <td>2546892.11</td>\n",
       "      <td>7201648.04</td>\n",
       "      <td>6930286.58</td>\n",
       "      <td>...</td>\n",
       "      <td>369474.87</td>\n",
       "      <td>1626980.56</td>\n",
       "      <td>6.670794e+08</td>\n",
       "      <td>10302926.4</td>\n",
       "      <td>494447.2</td>\n",
       "      <td>3904174.0</td>\n",
       "      <td>294690.746</td>\n",
       "      <td>2660812.946</td>\n",
       "      <td>1.350945e+09</td>\n",
       "      <td>5.667758e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20000</td>\n",
       "      <td>20</td>\n",
       "      <td>3412051.71</td>\n",
       "      <td>6261533.34</td>\n",
       "      <td>1725123.90</td>\n",
       "      <td>8619587.99</td>\n",
       "      <td>107764.62</td>\n",
       "      <td>2516342.15</td>\n",
       "      <td>15881935.84</td>\n",
       "      <td>7038621.18</td>\n",
       "      <td>...</td>\n",
       "      <td>360388.72</td>\n",
       "      <td>1455366.93</td>\n",
       "      <td>2.757284e+09</td>\n",
       "      <td>7913321.8</td>\n",
       "      <td>400458.2</td>\n",
       "      <td>3669802.8</td>\n",
       "      <td>355983.590</td>\n",
       "      <td>2923234.584</td>\n",
       "      <td>2.320653e+09</td>\n",
       "      <td>4.252196e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30000</td>\n",
       "      <td>20</td>\n",
       "      <td>2987325.82</td>\n",
       "      <td>5147554.41</td>\n",
       "      <td>2597924.96</td>\n",
       "      <td>8275803.16</td>\n",
       "      <td>82897.48</td>\n",
       "      <td>2087834.89</td>\n",
       "      <td>19820452.32</td>\n",
       "      <td>7044692.70</td>\n",
       "      <td>...</td>\n",
       "      <td>319890.13</td>\n",
       "      <td>1307907.29</td>\n",
       "      <td>6.277202e+09</td>\n",
       "      <td>7275263.0</td>\n",
       "      <td>727355.6</td>\n",
       "      <td>8246784.8</td>\n",
       "      <td>353742.482</td>\n",
       "      <td>2728613.477</td>\n",
       "      <td>3.372815e+09</td>\n",
       "      <td>4.985831e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40000</td>\n",
       "      <td>20</td>\n",
       "      <td>3717455.12</td>\n",
       "      <td>5243979.08</td>\n",
       "      <td>3450599.02</td>\n",
       "      <td>7734008.96</td>\n",
       "      <td>112366.11</td>\n",
       "      <td>2559911.11</td>\n",
       "      <td>25808129.34</td>\n",
       "      <td>7087528.32</td>\n",
       "      <td>...</td>\n",
       "      <td>422855.56</td>\n",
       "      <td>1365134.79</td>\n",
       "      <td>1.178490e+10</td>\n",
       "      <td>7722078.2</td>\n",
       "      <td>516536.4</td>\n",
       "      <td>4488748.6</td>\n",
       "      <td>410364.530</td>\n",
       "      <td>3007573.881</td>\n",
       "      <td>4.287349e+09</td>\n",
       "      <td>5.535492e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50000</td>\n",
       "      <td>20</td>\n",
       "      <td>4669801.20</td>\n",
       "      <td>5245320.74</td>\n",
       "      <td>4365054.36</td>\n",
       "      <td>8012003.86</td>\n",
       "      <td>114761.06</td>\n",
       "      <td>2757335.80</td>\n",
       "      <td>33218271.86</td>\n",
       "      <td>7100801.98</td>\n",
       "      <td>...</td>\n",
       "      <td>297103.62</td>\n",
       "      <td>1034083.87</td>\n",
       "      <td>2.540135e+10</td>\n",
       "      <td>24746986.8</td>\n",
       "      <td>424197.4</td>\n",
       "      <td>8857106.2</td>\n",
       "      <td>412987.833</td>\n",
       "      <td>2774710.565</td>\n",
       "      <td>5.893922e+09</td>\n",
       "      <td>6.700830e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>60000</td>\n",
       "      <td>20</td>\n",
       "      <td>5296804.84</td>\n",
       "      <td>5731909.75</td>\n",
       "      <td>4578323.31</td>\n",
       "      <td>8047922.16</td>\n",
       "      <td>96016.01</td>\n",
       "      <td>2224244.43</td>\n",
       "      <td>38010596.30</td>\n",
       "      <td>6692867.72</td>\n",
       "      <td>...</td>\n",
       "      <td>295778.19</td>\n",
       "      <td>1390522.96</td>\n",
       "      <td>3.690761e+10</td>\n",
       "      <td>36499633.2</td>\n",
       "      <td>381444.4</td>\n",
       "      <td>4538409.8</td>\n",
       "      <td>400499.100</td>\n",
       "      <td>2608809.617</td>\n",
       "      <td>5.962499e+09</td>\n",
       "      <td>6.859362e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>70000</td>\n",
       "      <td>20</td>\n",
       "      <td>5076857.11</td>\n",
       "      <td>4683326.08</td>\n",
       "      <td>5794457.36</td>\n",
       "      <td>8014643.13</td>\n",
       "      <td>78869.55</td>\n",
       "      <td>2020267.91</td>\n",
       "      <td>40556381.84</td>\n",
       "      <td>6953347.44</td>\n",
       "      <td>...</td>\n",
       "      <td>311670.39</td>\n",
       "      <td>1062952.36</td>\n",
       "      <td>5.118199e+10</td>\n",
       "      <td>39077798.8</td>\n",
       "      <td>486232.8</td>\n",
       "      <td>2917980.6</td>\n",
       "      <td>542694.180</td>\n",
       "      <td>3038352.580</td>\n",
       "      <td>7.880144e+09</td>\n",
       "      <td>7.636184e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>80000</td>\n",
       "      <td>20</td>\n",
       "      <td>6817160.31</td>\n",
       "      <td>5749304.94</td>\n",
       "      <td>6753651.55</td>\n",
       "      <td>8162628.19</td>\n",
       "      <td>87658.71</td>\n",
       "      <td>2095853.40</td>\n",
       "      <td>50931398.32</td>\n",
       "      <td>7887529.06</td>\n",
       "      <td>...</td>\n",
       "      <td>472910.05</td>\n",
       "      <td>1392389.67</td>\n",
       "      <td>6.789900e+10</td>\n",
       "      <td>33089260.6</td>\n",
       "      <td>515857.0</td>\n",
       "      <td>3233960.0</td>\n",
       "      <td>449705.629</td>\n",
       "      <td>2523107.487</td>\n",
       "      <td>8.692141e+09</td>\n",
       "      <td>8.724585e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>90000</td>\n",
       "      <td>20</td>\n",
       "      <td>6786482.00</td>\n",
       "      <td>5381216.38</td>\n",
       "      <td>7470260.19</td>\n",
       "      <td>7676873.22</td>\n",
       "      <td>92142.78</td>\n",
       "      <td>2321904.72</td>\n",
       "      <td>52956586.80</td>\n",
       "      <td>7199329.08</td>\n",
       "      <td>...</td>\n",
       "      <td>567072.46</td>\n",
       "      <td>1792351.27</td>\n",
       "      <td>8.949009e+10</td>\n",
       "      <td>39015719.0</td>\n",
       "      <td>692149.4</td>\n",
       "      <td>3786643.2</td>\n",
       "      <td>469702.901</td>\n",
       "      <td>2507444.073</td>\n",
       "      <td>9.777973e+09</td>\n",
       "      <td>1.071158e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>100000</td>\n",
       "      <td>20</td>\n",
       "      <td>7762776.87</td>\n",
       "      <td>5912820.19</td>\n",
       "      <td>8208674.65</td>\n",
       "      <td>7851123.79</td>\n",
       "      <td>94323.88</td>\n",
       "      <td>2243208.52</td>\n",
       "      <td>61432439.10</td>\n",
       "      <td>7763827.66</td>\n",
       "      <td>...</td>\n",
       "      <td>523807.84</td>\n",
       "      <td>1664374.66</td>\n",
       "      <td>1.156359e+11</td>\n",
       "      <td>82607634.6</td>\n",
       "      <td>624393.2</td>\n",
       "      <td>3220083.4</td>\n",
       "      <td>557645.332</td>\n",
       "      <td>2829119.802</td>\n",
       "      <td>1.133990e+10</td>\n",
       "      <td>1.030080e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_count  column_count  drop_na_pd  drop_na_spark  fill_na_pd  \\\n",
       "0      10000            20  2273045.35     6127367.31   930211.45   \n",
       "1      20000            20  3412051.71     6261533.34  1725123.90   \n",
       "2      30000            20  2987325.82     5147554.41  2597924.96   \n",
       "3      40000            20  3717455.12     5243979.08  3450599.02   \n",
       "4      50000            20  4669801.20     5245320.74  4365054.36   \n",
       "5      60000            20  5296804.84     5731909.75  4578323.31   \n",
       "6      70000            20  5076857.11     4683326.08  5794457.36   \n",
       "7      80000            20  6817160.31     5749304.94  6753651.55   \n",
       "8      90000            20  6786482.00     5381216.38  7470260.19   \n",
       "9     100000            20  7762776.87     5912820.19  8208674.65   \n",
       "\n",
       "   fill_na_spark   group_pd  group_spark  group_sum_pd  group_sum_spark  ...  \\\n",
       "0    11401758.28  100231.27   2546892.11    7201648.04       6930286.58  ...   \n",
       "1     8619587.99  107764.62   2516342.15   15881935.84       7038621.18  ...   \n",
       "2     8275803.16   82897.48   2087834.89   19820452.32       7044692.70  ...   \n",
       "3     7734008.96  112366.11   2559911.11   25808129.34       7087528.32  ...   \n",
       "4     8012003.86  114761.06   2757335.80   33218271.86       7100801.98  ...   \n",
       "5     8047922.16   96016.01   2224244.43   38010596.30       6692867.72  ...   \n",
       "6     8014643.13   78869.55   2020267.91   40556381.84       6953347.44  ...   \n",
       "7     8162628.19   87658.71   2095853.40   50931398.32       7887529.06  ...   \n",
       "8     7676873.22   92142.78   2321904.72   52956586.80       7199329.08  ...   \n",
       "9     7851123.79   94323.88   2243208.52   61432439.10       7763827.66  ...   \n",
       "\n",
       "   filter_less_10_pd  filter_less_10_spark       join_pd  join_spark  \\\n",
       "0          369474.87            1626980.56  6.670794e+08  10302926.4   \n",
       "1          360388.72            1455366.93  2.757284e+09   7913321.8   \n",
       "2          319890.13            1307907.29  6.277202e+09   7275263.0   \n",
       "3          422855.56            1365134.79  1.178490e+10   7722078.2   \n",
       "4          297103.62            1034083.87  2.540135e+10  24746986.8   \n",
       "5          295778.19            1390522.96  3.690761e+10  36499633.2   \n",
       "6          311670.39            1062952.36  5.118199e+10  39077798.8   \n",
       "7          472910.05            1392389.67  6.789900e+10  33089260.6   \n",
       "8          567072.46            1792351.27  8.949009e+10  39015719.0   \n",
       "9          523807.84            1664374.66  1.156359e+11  82607634.6   \n",
       "\n",
       "   mul_build_pd  mul_build_spark  mul_col_pd  mul_col_spark  pd_to_pyspark  \\\n",
       "0      494447.2        3904174.0  294690.746    2660812.946   1.350945e+09   \n",
       "1      400458.2        3669802.8  355983.590    2923234.584   2.320653e+09   \n",
       "2      727355.6        8246784.8  353742.482    2728613.477   3.372815e+09   \n",
       "3      516536.4        4488748.6  410364.530    3007573.881   4.287349e+09   \n",
       "4      424197.4        8857106.2  412987.833    2774710.565   5.893922e+09   \n",
       "5      381444.4        4538409.8  400499.100    2608809.617   5.962499e+09   \n",
       "6      486232.8        2917980.6  542694.180    3038352.580   7.880144e+09   \n",
       "7      515857.0        3233960.0  449705.629    2523107.487   8.692141e+09   \n",
       "8      692149.4        3786643.2  469702.901    2507444.073   9.777973e+09   \n",
       "9      624393.2        3220083.4  557645.332    2829119.802   1.133990e+10   \n",
       "\n",
       "   pyspark_to_pd  \n",
       "0   5.667758e+08  \n",
       "1   4.252196e+08  \n",
       "2   4.985831e+08  \n",
       "3   5.535492e+08  \n",
       "4   6.700830e+08  \n",
       "5   6.859362e+08  \n",
       "6   7.636184e+08  \n",
       "7   8.724585e+08  \n",
       "8   1.071158e+09  \n",
       "9   1.030080e+09  \n",
       "\n",
       "[10 rows x 24 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_statistics_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_statistics_small.to_csv(\"data/time_statistics_small.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-science_dublin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
